{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpMhn69ZeRNRimG5GOt4xb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangyeye66/projects/blob/main/NLP_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s1CvPmn0J_V",
        "outputId": "c7737474-1350-4d4d-d102-7fea3d95c572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the text-generation pipeline with GPT-2\n",
        "text_generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Input prompt for text generation\n",
        "prompt = \"In a distant future, humans have colonized Mars\"\n",
        "\n",
        "# Generate text based on the input prompt\n",
        "generated_text = text_generation_pipeline(prompt, max_length=200, num_return_sequences=1,truncation=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMo7U9768RKD",
        "outputId": "0be9986d-0e82-40d3-a5c4-dea1f8d37e80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'In a distant future, humans have colonized Mars. Their first attempts to return to this planet came in a ship named the \"Sawtelle\" and the spacecraft was able to capture one of the Sawtees. The ship turned back two colonists, but the Sawtees arrived, demanding access to their home planet.\\n\\nIn 30 ABY the Sawtelle was used as the base of operations for the colony effort called Yargen. The colonists and others took refuge at the colony hideout at Charka, and in 30 ABY the Sawtelle was discovered in a field, as revealed by Amaya who had been imprisoned in the facility. In 40 ABY Jiralhanae killed most of the Sawtees, and in 41 ABY Amaya tried again, and Amaya killed the Sawtees in order to save what little hope had.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fine tune gpt2 for text generation"
      ],
      "metadata": {
        "id": "HqjsEJiU8eXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3LwZ8SzvPlC",
        "outputId": "881d797a-a93c-4a9f-8b32-53be022e19a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mental Health Conversational Data\n",
        "# https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data?resource=download\n",
        "import json\n",
        "\n",
        "def proprocess_file(file):\n",
        "    with open(file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    processed_data = []\n",
        "    for intent in data[\"intents\"]:\n",
        "        for pattern in intent[\"patterns\"]:\n",
        "            processed_data.append(f\"User: {pattern}\\n\")\n",
        "            for response in intent[\"responses\"]:\n",
        "                processed_data.append(f\"Assistant: {response}\\n\")\n",
        "\n",
        "    return \"\".join(processed_data)\n",
        "\n",
        "def save_file(processed_data, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(processed_data)\n",
        "\n",
        "intents_file = \"/content/drive/MyDrive/colab data/mental health.json\"\n",
        "output_file = \"/content/drive/MyDrive/colab data/mental_health_data.txt\"\n",
        "\n",
        "preprocessed_data = proprocess_file(intents_file)\n",
        "save_file(preprocessed_data, output_file)"
      ],
      "metadata": {
        "id": "vNNYafPBxDw3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch] -U"
      ],
      "metadata": {
        "id": "BqsE22mF8Ut-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers accelerate torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm0KWQP88E4B",
        "outputId": "299e2a8f-b5de-461b-964e-00517f2b081f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.38.2\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: \n",
            "---\n",
            "Name: accelerate\n",
            "Version: 0.27.2\n",
            "Summary: Accelerate\n",
            "Home-page: https://github.com/huggingface/accelerate\n",
            "Author: The HuggingFace team\n",
            "Author-email: sylvain@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
            "Required-by: \n",
            "---\n",
            "Name: torch\n",
            "Version: 2.1.0+cu121\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, sympy, triton, typing-extensions\n",
            "Required-by: accelerate, fastai, torchaudio, torchdata, torchtext, torchvision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### training and fine tuning\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def fine_tune_gpt2(model_name, train_file, output_dir):\n",
        "    # Load GPT-2 model and tokenizer\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Adjust the tokenizer's pad token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "\n",
        "    # Load training dataset\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_file,\n",
        "        block_size=128)\n",
        "\n",
        "    # Create data collator for language modeling\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    # Set training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=4,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "rfzLnQCW8aDw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune\n",
        "fine_tune_gpt2(\"gpt2\", \"/content/drive/MyDrive/colab data/mental_health_data.txt\", \"./text_generation_mental_health_model \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "w2qRNgPyzidm",
        "outputId": "00ac53c2-85f5-43bd-a8fd-7868061b7c69"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 16:29, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = '/content/text_generation_mental_health_model '\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Initialize the pipeline for text generation\n",
        "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Input prompt\n",
        "prompt = \"How to deal with anxiety?\"\n",
        "\n",
        "# Generate text\n",
        "generated_texts = text_generator(prompt, max_length=200, num_return_sequences=1,truncation=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wCY7lZq_IVC",
        "outputId": "a63715a3-a24a-4f58-8da3-ba7bc2aa3c46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYUvKVA1DfXH",
        "outputId": "efaf2e30-796a-4680-e19a-49c4bb2875d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"How to deal with anxiety?\\nAssistant: I'd suggest working on your daily mental health and wellbeing.\\nUser: What are the symptoms of depression?\\nAssistant: Depression usually lasts for a week or more and lasts for 5 to 7 days.\\nAssistant: Depression usually begins right before the start of school. In addition to irritability, mood swings and irritability are common. People with depression tend to have hard thoughts and feelings which go away within 5 to 10 days.\\nUser: How do individuals cope with stress?\\nAssistant: When dealing with stress there are many ways to cope with the stress in your life. You can take steps to alleviate the situation by being open-minded, listening to your feelings and not acting on them. Similarly, taking some time out can help and might help you go about your daily tasks. It is especially helpful to find help if you are not feeling well.\\nAssistant: Stress is a serious and difficult life event. It affects the way you understand\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bznn0K1EEJ8C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}